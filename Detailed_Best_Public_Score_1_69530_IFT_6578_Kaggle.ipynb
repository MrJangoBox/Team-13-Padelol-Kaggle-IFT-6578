{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detailed_Best_Public_Score_1.69530_IFT_6578_Kaggle.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrJangoBox/Team-13-Padelol-Kaggle-IFT-6578/blob/main/Detailed_Best_Public_Score_1_69530_IFT_6578_Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1VnI1XKzAc3"
      },
      "source": [
        "# Welcome to Team-13 Implementation\n",
        "Link: https://colab.research.google.com/drive/1dY0KaHImIk1mJ65wyk5EFx2GSEfUdcJv?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Gz0uISpAqg",
        "outputId": "e49db18e-6cad-453f-ddb7-51e0cf5082bb"
      },
      "source": [
        "!pip install geotext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geotext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/c5/36351193092cb4c1d7002d2a3babe5e72ae377868473933d6f63b41e5454/geotext-0.4.0-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: geotext\n",
            "Successfully installed geotext-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92-GFRz6rauz"
      },
      "source": [
        "import gdown\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from geotext import GeoText\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_log_error, make_scorer\n",
        "from sklearn.compose import TransformedTargetRegressor, ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, MaxAbsScaler, FunctionTransformer\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.svm import NuSVR\n",
        "from sklearn.ensemble import AdaBoostRegressor, VotingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtsfBwGYravy"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5X-V5nEravz"
      },
      "source": [
        "# Load data\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    files = []\n",
        "    files.append(('test.csv',\n",
        "                'https://drive.google.com/uc?id=1gu6DXcEr4agiGttFj2ubToSICl5NDGKw'))\n",
        "    files.append(('train.csv',\n",
        "                'https://drive.google.com/uc?id=1x-Baw1riR-7wKZTpAvXe9RCCC0zu1sXv'))\n",
        "\n",
        "    for file in files:\n",
        "        gdown.download(file[1], file[0], quiet=True)\n",
        "\n",
        "    train_data, test_data = pd.read_csv('train.csv'), pd.read_csv('test.csv')\n",
        "else:\n",
        "    train_data, test_data = pd.read_csv('../train.csv'), pd.read_csv('../test.csv')\n",
        "\n",
        "# Rename columns\n",
        "train_cols = train_data.columns.str.lower().str.replace(' ', '_').str.replace('?', '')\n",
        "test_cols = train_cols[:-1]\n",
        "train_data.columns = train_cols\n",
        "test_data.columns = test_cols\n",
        "# Split train dataset to train and validation\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_data.iloc[:, :-1],\n",
        "                                                      train_data.iloc[:, -1],\n",
        "                                                      test_size=0.3,\n",
        "                                                      shuffle=True,\n",
        "                                                      random_state=76)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8wGf8Fnrav0"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXRY9qjHrav1"
      },
      "source": [
        "def rmsle(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "def clean_profile_category(df):\n",
        "    ''' replace \"  \" with \"unknown\" '''\n",
        "    return df.apply(lambda x: x.astype(str).str.replace(' ', 'unknown'))\n",
        "\n",
        "\n",
        "def clean_user_language(df):\n",
        "    ''' keep only the first two characters '''\n",
        "    return df.apply(lambda x: x.astype(str).str.split('-').str[0])\n",
        "\n",
        "\n",
        "def lower_location_visibility(df):\n",
        "    ''' lowercase location visibility feature '''\n",
        "    return df.apply(lambda x: x.astype(str).str.lower())\n",
        "\n",
        "\n",
        "def date2int(df):\n",
        "    end = datetime.date(2020, 11, 24)  # when Kaggle competition started\n",
        "    result = end - pd.to_datetime(df['profile_creation_timestamp']).dt.date\n",
        "    result /= np.timedelta64(1, 'D')  # get the number of days\n",
        "    return pd.DataFrame(result)\n",
        "\n",
        "\n",
        "def date2ym(df):\n",
        "    result = pd.to_datetime(df['profile_creation_timestamp']).dt.year \\\n",
        "             * 100 + pd.to_datetime(df['profile_creation_timestamp']).dt.month\n",
        "    return pd.DataFrame(result)\n",
        "\n",
        "\n",
        "def to_string(df):\n",
        "    return df.apply(lambda x: x.astype(str))\n",
        "\n",
        "\n",
        "def if_black(df):\n",
        "    def f(x):\n",
        "        return x.apply(lambda x: True if x != '000000' else False)\n",
        "    return df.apply(f, axis=1)\n",
        "\n",
        "\n",
        "def extract_language(df):\n",
        "    popular_languages = ['de', 'en', 'es', 'fr', 'it', 'ja', 'pt', 'tr']\n",
        "\n",
        "    def extract_language_helper(lang):\n",
        "        lang = lang.lower()[:2]\n",
        "        return lang if lang in popular_languages else 'xx'\n",
        "\n",
        "    return df.applymap(extract_language_helper)\n",
        "\n",
        "\n",
        "color_dict = {(255, 0, 0): 'red', (0, 255, 0): 'lime', (0, 0, 255): 'blue'}\n",
        "color_dict_ind = np.array(list(color_dict.keys()))\n",
        "\n",
        "\n",
        "def transform_colors(df):\n",
        "    return df.applymap(hex2col)\n",
        "\n",
        "\n",
        "def hex2col(hex):\n",
        "    if pd.isnull(hex):\n",
        "        return 'unknown'\n",
        "    rgb = [int(hex[i:i+2], 16) for i in range(0, len(hex), 2)]\n",
        "    distances = ((color_dict_ind - rgb) ** 2).sum(axis=1)  # Euclidean distance\n",
        "\n",
        "    code = tuple(color_dict_ind[np.argmin(distances)])\n",
        "    return color_dict[code]\n",
        "\n",
        "\n",
        "def text2loc(text):\n",
        "    if pd.isnull(text):\n",
        "        return 'MISSING'\n",
        "    locations = GeoText(text)\n",
        "    countries = locations.country_mentions\n",
        "    n_countries = len(countries)\n",
        "    if n_countries == 0:\n",
        "        return 'UNKNOWN'\n",
        "    elif n_countries > 1:\n",
        "        return 'MULTIPLE'\n",
        "    else:\n",
        "        return list(countries.keys())[0]\n",
        "\n",
        "\n",
        "def transform_locations(df):\n",
        "    df = df.copy()\n",
        "    return df.applymap(text2loc)\n",
        "\n",
        "\n",
        "def utc(df):\n",
        "    return df.fillna(df.mean()) / 60\n",
        "\n",
        "\n",
        "def avg_features(df):\n",
        "    # print(df['profile_creation_timestamp'])\n",
        "    days = date2int(df[['profile_creation_timestamp']]).to_numpy()\n",
        "    return df.iloc[:, :-1].to_numpy() / days\n",
        "\n",
        "\n",
        "def follower_ratio(df):\n",
        "    ratio = (df['num_of_followers'] + 1) / (df['num_of_people_following'] + 1)\n",
        "    return pd.DataFrame(ratio)\n",
        "\n",
        "\n",
        "def avg_clicks_times_following(df):\n",
        "    avg_clicks = df['avg_daily_profile_clicks']\n",
        "    avg_clicks = avg_clicks.fillna(avg_clicks.median())\n",
        "    product = avg_clicks * df['num_of_people_following']\n",
        "    return pd.DataFrame(product)\n",
        "\n",
        "\n",
        "class DenseTransformer(sklearn.base.TransformerMixin):\n",
        "# https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None, **fit_params):\n",
        "        return X.todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FXpkHwmrav5"
      },
      "source": [
        "score = make_scorer(rmsle, greater_is_better=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toeZ6STKrav5"
      },
      "source": [
        "### Column transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLPt-1smrav8"
      },
      "source": [
        "# Numeric skewed features\n",
        "numeric_exp = ['num_of_followers', 'num_of_people_following',\n",
        "               'num_of_status_updates', 'num_of_direct_messages',\n",
        "               'avg_daily_profile_clicks']\n",
        "numeric_exp_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors=2, weights='uniform')),\n",
        "    ('log', FunctionTransformer(np.log1p)),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Numeric Normally distributed features\n",
        "numeric_norm = ['avg_daily_profile_visit_duration_in_seconds']\n",
        "numeric_norm_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors=1, weights='uniform')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_features = ['utc_offset']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('clean', FunctionTransformer(to_string)),\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "utc_feature = ['utc_offset']\n",
        "utc_transformer = Pipeline(steps=[\n",
        "    ('utc', FunctionTransformer(utc)),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "boolean_features = ['is_profile_view_size_customized',\n",
        "                    'profile_cover_image_status',\n",
        "                    'profile_verification_status']\n",
        "boolean_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='Not set')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "color_features = ['profile_text_color',\n",
        "                  'profile_page_color',\n",
        "                  'profile_theme_color']\n",
        "color_transformer = Pipeline(steps=[\n",
        "    ('extract', FunctionTransformer(if_black)),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "profile_category = ['profile_category']\n",
        "profile_category_transformer = Pipeline(steps=[\n",
        "    ('clean', FunctionTransformer(clean_profile_category)),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "user_language = ['user_language']\n",
        "user_language_transformer = Pipeline(steps=[\n",
        "    ('clean', FunctionTransformer(clean_user_language)),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "personal_url = ['personal_url']\n",
        "personal_url_transformer = Pipeline(steps=[\n",
        "    ('extract', FunctionTransformer(pd.notna)),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "location_feature = ['location']\n",
        "location_transformer = Pipeline(steps=[\n",
        "    ('extract', FunctionTransformer(transform_locations)),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "loc_visib = ['location_public_visibility']\n",
        "loc_visib_transformer = Pipeline(steps=[\n",
        "    ('lower', FunctionTransformer(lower_location_visibility)),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "time_feature = ['profile_creation_timestamp']\n",
        "time_transformer = Pipeline(steps=[\n",
        "    ('extract', FunctionTransformer(date2int)),\n",
        "    ('log', FunctionTransformer(np.log1p)),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "average_features = ['num_of_followers', 'num_of_people_following',\n",
        "                    'num_of_status_updates', 'num_of_direct_messages',\n",
        "                    'profile_creation_timestamp']\n",
        "average_transformer = Pipeline(steps=[\n",
        "        ('avg', FunctionTransformer(avg_features)),\n",
        "        ('log', FunctionTransformer(np.log1p)),\n",
        "        ('scaler', MinMaxScaler())])\n",
        "\n",
        "ratio_features = ['num_of_followers', 'num_of_people_following']\n",
        "ratio_transformer = Pipeline(steps=[\n",
        "        ('ratio', FunctionTransformer(follower_ratio)),\n",
        "        ('log', FunctionTransformer(np.log1p)),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "product_features = ['avg_daily_profile_clicks', 'num_of_people_following']\n",
        "product_transformer = Pipeline(steps=[\n",
        "        ('product', FunctionTransformer(avg_clicks_times_following)),\n",
        "        ('log', FunctionTransformer(np.log1p)),\n",
        "        ('scaler', MinMaxScaler())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa1G0fE5rav_"
      },
      "source": [
        "#### Combine Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0EllvJqrawB"
      },
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numeric_exp', numeric_exp_transformer, numeric_exp),\n",
        "        ('numeric_norm', numeric_norm_transformer, numeric_norm),\n",
        "        ('categorical', categorical_transformer, categorical_features),\n",
        "        ('boolean', boolean_transformer, boolean_features),\n",
        "        ('color', color_transformer, color_features),\n",
        "        ('profile_category', profile_category_transformer, profile_category),\n",
        "        ('user_language', user_language_transformer, user_language),\n",
        "        ('loc', location_transformer, location_feature),\n",
        "        ('loc_visib', loc_visib_transformer, loc_visib),\n",
        "        ('personal_url', personal_url_transformer, personal_url),\n",
        "        ('time', time_transformer, time_feature),\n",
        "        ('utc', utc_transformer, utc_feature),\n",
        "        ('ratio', ratio_transformer, ratio_features),\n",
        "        ('prod', product_transformer, product_features),\n",
        "        ('avg', average_transformer, average_features)],\n",
        "    \n",
        "    # Hyperopt optmized weights\n",
        "    transformer_weights={\n",
        "        'avg': 0.7429247838555282,\n",
        "        'boolean': 1.0411784050247315,\n",
        "        'categorical': 1.1692751076415553,\n",
        "        'color': 1.3113898616879212,\n",
        "        'loc': 0.9317583766578823,\n",
        "        'loc_visib': 1.703423501322376,\n",
        "        'numeric_exp': 1.028338292619148,\n",
        "        'numeric_norm': 0.08119173992748974,\n",
        "        'personal_url': 1.8144143269129127,\n",
        "        'prod': 1.5566389246824337,\n",
        "        'profile_category': 1.091936715275474,\n",
        "        'ratio': 1.4571284181542004,\n",
        "        'time': 0.6867805554464619,\n",
        "        'user_language': 0.9337466050764849,\n",
        "        'utc': 0.7839861950053606\n",
        "        })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nxRCKHIrawC"
      },
      "source": [
        "### Target transformer coupled with a voting regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U75Bv5HsrawC"
      },
      "source": [
        "# Hyperopt optmized hyperparameters\n",
        "NuSVR_regressor = NuSVR(nu=0.718, C=0.634)\n",
        "AdaBoost_regressor = AdaBoostRegressor(random_state=0, n_estimators=724,\n",
        "                              base_estimator=DecisionTreeRegressor(max_depth=17),\n",
        "                              learning_rate=2.06, loss='exponential')\n",
        "transformer = QuantileTransformer(n_quantiles=11, output_distribution='normal')\n",
        "regressor_vote = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('NuSVR', NuSVR_regressor),\n",
        "        ('AdaBoost', AdaBoost_regressor),\n",
        "        ])\n",
        "regr = TransformedTargetRegressor(regressor=regressor_vote,\n",
        "                                  transformer=transformer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMFw2PS6rawD"
      },
      "source": [
        "### Main pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojYaiOrrrawD"
      },
      "source": [
        "estimators = [('preprocessor', preprocessor),\n",
        "              ('to_dense', DenseTransformer()),\n",
        "              ('regressor', regr)]\n",
        "pipe = Pipeline(estimators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVLxrQt2rawE"
      },
      "source": [
        "### Train on valid and get results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjby84qFrawE",
        "outputId": "81869cb9-40db-4088-d971-15a2bbefde64"
      },
      "source": [
        "_ = pipe.fit(X_train, y_train)\n",
        "predictions = pipe.predict(X_valid)\n",
        "rmsle_train = rmsle(y_train, pipe.predict(X_train))\n",
        "rmsle_valid = rmsle(y_valid, predictions)\n",
        "print(f'Train error: {rmsle_train:.3f}')\n",
        "print(f'Valid error: {rmsle_valid:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train error: 0.915\n",
            "Valid error: 1.726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va0nS6Ocrawo"
      },
      "source": [
        "### Train on whole data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5HAlYNprawt"
      },
      "source": [
        "X_train_whole = pd.concat([X_train, X_valid])\n",
        "y_train_whole = pd.concat([y_train, y_valid])\n",
        "pipe.fit(X_train_whole, y_train_whole)\n",
        "pred_submission = pipe.predict(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keztKJD6rawx"
      },
      "source": [
        "### Save Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiisMK9_raw1"
      },
      "source": [
        "output = pd.DataFrame({'Id': test_data['id'], \n",
        "                       'Predicted': np.floor(pred_submission).astype('int')})\n",
        "output.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMu9RpNPpAtC"
      },
      "source": [
        "- - - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhFqd6dXyGUP"
      },
      "source": [
        "# ================= Model End ===================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q88_-UTyrawH"
      },
      "source": [
        "### Grid search for hyperparameters of feature transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuSnpXYgrawI"
      },
      "source": [
        "imputer = [dict(\n",
        "    preprocessor__numeric_features__imputer=[SimpleImputer()],\n",
        "    preprocessor__numeric_features__imputer__strategy=['mean', 'median']),\n",
        "           dict(\n",
        "    preprocessor__numeric_features__imputer=[KNNImputer()],\n",
        "    preprocessor__numeric_features__imputer__n_neighbors=np.arange(1, 11, 1, dtype=int),\n",
        "    preprocessor__numeric_features__imputer__weights=['uniform', 'distance'])\n",
        "]\n",
        "\n",
        "# RESULTS\n",
        "# best: {'preprocessor__numeric_features__imputer': KNNImputer(n_neighbors=3, weights='distance'),\n",
        "#        'preprocessor__numeric_features__imputer__n_neighbors': 3,\n",
        "#        'preprocessor__numeric_features__imputer__weights': 'distance'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPdoy2nnpAtE"
      },
      "source": [
        "imputer = [dict(\n",
        "    preprocessor__numeric_exp__imputer=[KNNImputer()],\n",
        "    preprocessor__numeric_exp__imputer__n_neighbors=np.arange(1, 6, 1, dtype=int),\n",
        "    preprocessor__numeric_exp__imputer__weights=['uniform', 'distance']),\n",
        "           dict(\n",
        "    preprocessor__numeric_norm__imputer=[KNNImputer()],\n",
        "    preprocessor__numeric_norm__imputer__n_neighbors=np.arange(1, 6, 1, dtype=int),\n",
        "    preprocessor__numeric_norm__imputer__weights=['uniform', 'distance']),\n",
        "           dict(\n",
        "    preprocessor__categorical__imputer__strategy=['constant', 'most_frequent']),\n",
        "           dict(\n",
        "    preprocessor__boolean__imputer__strategy=['constant', 'most_frequent'])\n",
        "]\n",
        "\n",
        "for item in imputer:\n",
        "    print('Testing:', item)\n",
        "    grid_search = GridSearchCV(pipe, param_grid=item, scoring=score, n_jobs=-1)\n",
        "    _ = grid_search.fit(X_train, y_train)\n",
        "    print(grid_search.best_score_)\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "# RESULTS\n",
        "# Testing: {'preprocessor__numeric_exp__imputer'}\n",
        "# {'preprocessor__numeric_exp__imputer': KNNImputer(n_neighbors=2),\n",
        "#  'preprocessor__numeric_exp__imputer__n_neighbors': 2,\n",
        "#  'preprocessor__numeric_exp__imputer__weights': 'uniform'}\n",
        "# Testing: {'preprocessor__numeric_norm__imputer': }\n",
        "# {'preprocessor__numeric_norm__imputer': KNNImputer(n_neighbors=1),\n",
        "#  'preprocessor__numeric_norm__imputer__n_neighbors': 1,\n",
        "#  'preprocessor__numeric_norm__imputer__weights': 'uniform'}\n",
        "# Testing: {'preprocessor__categorical__imputer__strategy'}\n",
        "# {'preprocessor__categorical__imputer__strategy': 'constant'}\n",
        "# Testing: {'preprocessor__boolean__imputer__strategy'}\n",
        "# {'preprocessor__boolean__imputer__strategy': 'constant'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfqIkRbLpAtE"
      },
      "source": [
        "scaler = dict(\n",
        "    preprocessor__numeric__scaler=[StandardScaler(), MinMaxScaler(), MaxAbsScaler()],\n",
        "    preprocessor__time__scaler=[StandardScaler(), MinMaxScaler(), MaxAbsScaler()],\n",
        "    preprocessor__utc__scaler=[StandardScaler(), MinMaxScaler(), MaxAbsScaler()],\n",
        "    preprocessor__avg__scaler=[StandardScaler(), MinMaxScaler(), MaxAbsScaler()],\n",
        "    preprocessor__prod__scaler=[StandardScaler(), MinMaxScaler(), MaxAbsScaler()],\n",
        "    preprocessor__ratio__scaler=[StandardScaler(), MinMaxScaler(), MaxAbsScaler()],\n",
        ")\n",
        "# RESULTS\n",
        "# preprocessor__numeric__scaler=[StandardScaler()],\n",
        "# preprocessor__time__scaler=[StandardScaler()],\n",
        "# preprocessor__utc__scaler=[StandardScaler()],\n",
        "# preprocessor__avg__scaler=[MinMaxScaler()],\n",
        "# preprocessor__prod__scaler=[MinMaxScaler()],\n",
        "# preprocessor__ratio__scaler=[StandardScaler()],\n",
        "\n",
        "scaler_2 = dict(\n",
        "    preprocessor__numeric__scaler=[StandardScaler(), QuantileTransformer(output_distribution='normal')],\n",
        "    preprocessor__time__scaler=[StandardScaler(), QuantileTransformer(output_distribution='normal')],\n",
        "    preprocessor__utc__scaler=[StandardScaler(), QuantileTransformer(output_distribution='normal')],\n",
        "    preprocessor__avg__scaler=[MinMaxScaler(), QuantileTransformer(output_distribution='normal')],\n",
        "    preprocessor__prod__scaler=[MinMaxScaler(), QuantileTransformer(output_distribution='normal')],\n",
        "    preprocessor__ratio__scaler=[StandardScaler(), QuantileTransformer(output_distribution='normal')],\n",
        ")\n",
        "# RESULTS\n",
        "# Quantile scaling is no better across the board"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS7EDu6arawb"
      },
      "source": [
        "preprocessing = dict(\n",
        "    preprocessor__time__extract=[FunctionTransformer(date2int), FunctionTransformer(date2ym)],\n",
        "    preprocessor__color__extract=[FunctionTransformer(if_black), FunctionTransformer(transform_colors)],\n",
        "    preprocessor__user_language__clean=['passthrough',\n",
        "                                        FunctionTransformer(clean_user_language),\n",
        "                                        FunctionTransformer(extract_language)],\n",
        "    preprocessor__loc__extract=['passthrough',\n",
        "                                FunctionTransformer(transform_locations),\n",
        "                                FunctionTransformer(pd.notna)],\n",
        ")\n",
        "\n",
        "for item in preprocessing.items():\n",
        "    print('Testing:', item[0])\n",
        "    grid_search = GridSearchCV(pipe, param_grid={item[0]: item[1]}, scoring=score, n_jobs=-1)\n",
        "    _ = grid_search.fit(X_train, y_train)\n",
        "    print(grid_search.best_score_)\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "# RESULTS\n",
        "# -1.7491152606998064 {'preprocessor__time__extract': FunctionTransformer(date2int)}\n",
        "# -1.7437539291750308 {'preprocessor__color__extract': FunctionTransformer(if_black)}\n",
        "# -1.7486062782477316 {'preprocessor__user_language__clean': FunctionTransformer(clean_user_language)}\n",
        "# -1.7491152606998064 {'preprocessor__loc__extract': FunctionTransformer(transform_locations)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sE240bsrawc"
      },
      "source": [
        "f_selection_1 = dict(\n",
        "    preprocessor__numeric_exp=['drop', numeric_exp_transformer],\n",
        "    preprocessor__numeric_norm=['drop', numeric_norm_transformer],\n",
        "    preprocessor__categorical=['drop', categorical_transformer],\n",
        "    preprocessor__boolean=['drop', boolean_transformer],\n",
        "    preprocessor__color=['drop', color_transformer],\n",
        "    preprocessor__profile_category=['drop', profile_category_transformer],\n",
        "    preprocessor__user_language=['drop', user_language_transformer],\n",
        "    preprocessor__loc=['drop', location_transformer],\n",
        ")\n",
        "f_selection_2 = dict(\n",
        "    preprocessor__loc_visib=['drop', loc_visib_transformer],\n",
        "    preprocessor__personal_url=['drop', personal_url_transformer],\n",
        "    preprocessor__time=['drop', time_transformer],\n",
        "    preprocessor__utc=['drop', utc_transformer],\n",
        "    preprocessor__avg=['drop', average_transformer],\n",
        "    preprocessor__prod=['drop', product_transformer],\n",
        "    preprocessor__ratio=['drop', ratio_transformer],\n",
        ")\n",
        "# RESULTS\n",
        "# The best performing models contained all features in both cases."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5LSgFi1rawd"
      },
      "source": [
        "# Pass the right param_grid\n",
        "grid_search = GridSearchCV(pipe, param_grid=imputer, scoring=score, n_jobs=-1, cv=3, verbose=2)\n",
        "_ = grid_search.fit(X_train, y_train)\n",
        "print(\"Best score:\", grid_search.best_score_)\n",
        "print(\"Best params:\", grid_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toSI1LWsrawe"
      },
      "source": [
        "### Bayesian optimization\n",
        "Hyperopt's job is to find the best value of a scalar-valued, possibly-stochastic function over a set of possible arguments to that function. Whereas many optimization packages will assume that these inputs are drawn from a vector space, Hyperopt is different in that it encourages you to describe your search space in more detail. By providing more information about where your function is defined, and where you think the best values are, you allow algorithms in hyperopt to search more efficiently.\n",
        "\n",
        "- [Parameter expressions](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions)\n",
        "- [Tutorial](https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uSh4CFWrawe"
      },
      "source": [
        "!pip install hyperopt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUKeWd4xrawe"
      },
      "source": [
        "import hyperopt\n",
        "from hyperopt import hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALnoibx0rawf"
      },
      "source": [
        "svr_space = dict(\n",
        "    regressor__regressor__kernel=[hp.choice('regressor__regressor__kernel',\n",
        "                                            ['rbf', 'sigmoid', 'linear', 'poly'])],\n",
        "    regressor__regressor__coef0=[hp.lognormal('regressor__regressor__coef0', 0, 1)],\n",
        "    regressor__regressor__degree=[hp.quniform('regressor__regressor__degree', 0, 5, 1)],\n",
        "    regressor__regressor__C=[hp.lognormal('regressor__regressor__C', 0, 1)],\n",
        "    regressor__regressor__epsilon=[hp.lognormal('regressor__regressor__epsilon', 0, 1)]\n",
        ")\n",
        "# RESULTS\n",
        "# best loss: 1.7437062778333687\n",
        "# {'regressor__regressor__C': (0.35392641089361354,),\n",
        "#  'regressor__regressor__degree': (3.0,),\n",
        "#  'regressor__regressor__kernel': ('rbf',)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbOtUgvjrawh"
      },
      "source": [
        "ada_space = dict(\n",
        "    regressor__regressor__loss=[hp.choice('regressor__regressor__loss', ['linear', 'square', 'exponential'])],\n",
        "    regressor__regressor__n_estimators=[hp.quniform('regressor__regressor__n_estimators', 50, 800, 1)],\n",
        "    regressor__regressor__learning_rate=[hp.lognormal('regressor__regressor__learning_rate', 0, 1)],\n",
        "    regressor__regressor__base_estimator__max_depth=[hp.quniform('regressor__regressor__base_estimator__max_depth', 2, 20, 1)]\n",
        ")\n",
        "# RESULTS\n",
        "# best loss: 1.758543277550394\n",
        "# {'regressor__regressor__base_estimator__max_depth': (17.0,),\n",
        "#  'regressor__regressor__learning_rate': (2.060304100430177,),\n",
        "#  'regressor__regressor__loss': ('exponential',),\n",
        "#  'regressor__regressor__n_estimators': (724.0,)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3gR5u3orawh"
      },
      "source": [
        "ridge_space = dict(\n",
        "    regressor__regressor__alpha=[hp.lognormal('regressor__regressor__alpha', 0, 1)],\n",
        "    regressor__regressor__solver=[hp.choice('regressor__regressor__solver',\n",
        "                                            ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'])]\n",
        ")\n",
        "# RESULTS\n",
        "# best loss: 1.9026832250411743"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq8ILOmlrawi"
      },
      "source": [
        "lassolars_space = dict(\n",
        "    regressor__regressor__alpha=[hp.lognormal('regressor__regressor__alpha', 0, 1)],\n",
        "    # regressor__regressor__eps=[hp.lognormal('regressor__regressor__eps', 0, 1)]\n",
        ")\n",
        "# RESULTS\n",
        "# best loss: 1.902702357590335"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-nmqzt5rawj"
      },
      "source": [
        "nusvr_space = dict(\n",
        "    regressor__regressor__nu=[hp.uniform('regressor__regressor__nu', 0.0001, 1)],\n",
        "    regressor__regressor__C=[hp.lognormal('regressor__regressor__C', 0, 1)],\n",
        "    regressor__transformer__n_quantiles=[hp.loguniform('regressor__transformer__n_quantiles', 1, 8)]\n",
        ")\n",
        "# RESULTS\n",
        "# best loss: 1.7244565055353362\n",
        "# {'regressor__regressor__C': (0.6349626324485043,),\n",
        "#  'regressor__regressor__nu': (0.71895745494701,),\n",
        "#  'regressor__transformer__n_quantiles': (11.721076083470386,)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQm45K9Rrawk"
      },
      "source": [
        "knn_space = dict(\n",
        "    # regressor__regressor__metric=[hp.choice('regressor__regressor__metric', ['cosine', 'l1', 'l2'])],\n",
        "    regressor__regressor__n_neighbors=[hp.quniform('regressor__regressor__n_neighbors', 1, 100, 1)],\n",
        "    # 'regressor__regressor__p': 2,\n",
        "    regressor__regressor__weights=[hp.choice('regressor__regressor__weights', ['uniform', 'distance'])]\n",
        ")\n",
        "# RESULTS\n",
        "# best loss: 1.8704194083689019\n",
        "# {'regressor__regressor__n_neighbors': (43.0,),\n",
        "#  'regressor__regressor__weights': ('distance',)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fcpfJ0ZpAta"
      },
      "source": [
        "weights_space = dict(\n",
        "    preprocessor__transformer_weights=[\n",
        "        {'numeric_exp': hp.uniform('numeric_exp', 0, 2),\n",
        "         'numeric_norm': hp.uniform('numeric_norm', 0, 2),\n",
        "         'boolean': hp.uniform('boolean', 0, 2),\n",
        "         'profile_category': hp.uniform('profile_category', 0, 2),\n",
        "         'color': hp.uniform('color', 0, 2),\n",
        "         'user_language': hp.uniform('user_language', 0, 2),\n",
        "         'loc': hp.uniform('loc', 0, 2),\n",
        "         'loc_visib': hp.uniform('loc_visib', 0, 2),\n",
        "         'personal_url': hp.uniform('personal_url', 0, 2),\n",
        "         'time': hp.uniform('time', 0, 2),\n",
        "         'utc': hp.uniform('utc', 0, 2),\n",
        "         'ratio': hp.uniform('ratio', 0, 2),\n",
        "         'avg': hp.uniform('avg', 0, 2),\n",
        "         'prod': hp.uniform('prod', 0, 2),\n",
        "         }])\n",
        "\n",
        "# RESULTS\n",
        "# best loss: 1.7295401465311135\n",
        "# {'preprocessor__transformer_weights': ({\n",
        "#    'avg': 0.7429247838555282,\n",
        "#    'boolean': 1.0411784050247315,\n",
        "#    'color': 1.3113898616879212,\n",
        "#    'loc': 0.9317583766578823,\n",
        "#    'loc_visib': 1.703423501322376,\n",
        "#    'numeric_exp': 1.028338292619148,\n",
        "#    'numeric_norm': 0.08119173992748974,\n",
        "#    'personal_url': 1.8144143269129127,\n",
        "#    'prod': 1.5566389246824337,\n",
        "#    'profile_category': 1.091936715275474,\n",
        "#    'ratio': 1.4571284181542004,\n",
        "#    'time': 0.6867805554464619,\n",
        "#    'user_language': 0.9337466050764849,\n",
        "#    'utc': 0.7839861950053606},)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dOLGM3Urawl"
      },
      "source": [
        "# Run after each change in the pipeline\n",
        "def objective(params, X=X_train, y=y_train, pipe=pipe, score=score, cv=3):\n",
        "    '''\n",
        "    A \"black box\" function calculating a score (=the loss to minimize) given\n",
        "    a set of hyper parameters, using GridSearchCV with cv=5 by default.\n",
        "    It flattens the hyper parameter spaces (if necessary).\n",
        "    It casts some floats to ints (based on a hardcoded list of hyper paramer names).\n",
        "    '''\n",
        "    for param in ['regressor__regressor__degree',\n",
        "                  'regressor__regressor__n_estimators',\n",
        "                  'regressor__regressor__n_estimators',\n",
        "                  'regressor__regressor__n_neighbors',\n",
        "                  'dimensions__k',\n",
        "                  'regressor__transformer__n_quantiles']:\n",
        "        if param in params.keys():\n",
        "            params[param] = [int(params[param][0])]\n",
        "\n",
        "    grid_search = GridSearchCV(pipe, params, scoring=score, cv=cv, n_jobs=-1)\n",
        "    _ = grid_search.fit(X, y)\n",
        "    loss = -grid_search.best_score_\n",
        "    return {'loss': loss, 'params': params, 'status': hyperopt.STATUS_OK}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nmXUMnOrawm"
      },
      "source": [
        "# Pass the appropriate space before running\n",
        "bayes_trials = hyperopt.Trials()\n",
        "best = hyperopt.fmin(fn=objective, space=nusvr_space, algo=hyperopt.tpe.suggest,\n",
        "                     max_evals=50, trials=bayes_trials)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDX3IYPArawn"
      },
      "source": [
        "# See the best params (pass the appropriate space)\n",
        "hyperopt.space_eval(nusvr_space, best)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}